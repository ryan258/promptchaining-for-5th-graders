"""
Emergence Measurement Demo
===========================

Demonstrates scientific measurement of emergence in prompt chains.

This demo answers the question: "Do chains really unlock insights impossible
from single prompts?"

We test this by:
1. Running a chain approach on a topic
2. Running a mega-prompt baseline on the same topic
3. Measuring both outputs across multiple dimensions
4. Providing statistical validation
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from emergence_measurement import (
    measure_emergence,
    batch_measure,
    generate_report,
    quick_test
)

# Import some chain functions to test
try:
    from tools.learning.concept_simplifier import concept_simplifier
    from tools.learning.subject_connector import subject_connector
except ImportError:
    print("‚ö†Ô∏è  Note: Learning tools not available. Using natural_reasoning patterns instead.")
    from natural_reasoning import scientific_method, design_thinking

    # Wrap them to match expected signature
    def concept_simplifier(topic):
        # For demo purposes, create a simple wrapper
        return {"topic": topic, "explanation": "Concept simplification"}, {"total_tokens": 500}

    def subject_connector(topic):
        return {"subjects": topic, "connections": "Deep connections"}, {"total_tokens": 600}


def demo_single_comparison():
    """Demo: Compare chain vs baseline on a single topic."""
    print("\n" + "=" * 70)
    print("DEMO 1: SINGLE TOPIC COMPARISON")
    print("=" * 70)
    print("\nWe'll compare the concept_simplifier chain vs a single mega-prompt")
    print("on the topic 'Neural Networks'")
    print("\nThis reveals whether the chain's multi-step decomposition actually")
    print("produces better insights than asking for everything at once.")
    print()

    input("[Press Enter to run comparison]")

    comparison, metadata = measure_emergence(
        topic="Neural Networks",
        chain_function=concept_simplifier
    )

    print("\n" + "=" * 70)
    print("RESULTS")
    print("=" * 70)

    print(f"\nüèÜ Winner: {comparison['winner']}")
    print(f"\nüìä Analysis:")
    print(f"  {comparison['analysis']}")

    print(f"\nüìà Detailed Scores:")
    scores = comparison['scores'].get('scores', {})
    a_scores = scores.get('approach_a', {})  # Chain
    b_scores = scores.get('approach_b', {})  # Baseline

    dimensions = ['novelty', 'depth', 'coherence', 'pedagogical', 'actionability']
    print(f"\n  {'Dimension':<20} {'Chain':>8} {'Baseline':>8} {'Winner':>10}")
    print(f"  {'-'*50}")

    for dim in dimensions:
        chain_score = a_scores.get(dim, 0)
        baseline_score = b_scores.get(dim, 0)
        winner = "Chain" if chain_score > baseline_score else ("Baseline" if baseline_score > chain_score else "Tie")
        print(f"  {dim.capitalize():<20} {chain_score:>8}/10 {baseline_score:>8}/10 {winner:>10}")

    print(f"\nüí° Most Novel Insights:")
    qualitative = comparison['scores'].get('qualitative', {})
    print(f"  Chain: {qualitative.get('most_novel_insight_a', 'N/A')}")
    print(f"  Baseline: {qualitative.get('most_novel_insight_b', 'N/A')}")

    print(f"\n‚ö° Performance:")
    print(f"  Chain: {comparison['performance']['chain']['time_seconds']:.1f}s, {comparison['performance']['chain']['tokens']} tokens")
    print(f"  Baseline: {comparison['performance']['baseline']['time_seconds']:.1f}s, {comparison['performance']['baseline']['tokens']} tokens")


def demo_batch_measurement():
    """Demo: Measure across multiple topics for statistical significance."""
    print("\n" + "=" * 70)
    print("DEMO 2: BATCH MEASUREMENT (Statistical Validation)")
    print("=" * 70)
    print("\nTo truly validate emergence, we need to test across multiple topics.")
    print("This provides statistical confidence that chains consistently outperform.")
    print("\nWe'll test the concept_simplifier on 5 diverse topics:")

    topics = [
        "Quantum Computing",
        "Photosynthesis",
        "Supply Chain Management",
        "Emotional Intelligence",
        "Blockchain"
    ]

    for i, topic in enumerate(topics, 1):
        print(f"  {i}. {topic}")

    print()
    input("[Press Enter to run batch measurement - this will take a few minutes]")

    aggregate, individual_results = batch_measure(
        topics=topics,
        chain_function=concept_simplifier
    )

    print("\n" + "=" * 70)
    print("BATCH RESULTS")
    print("=" * 70)

    print(f"\nüìä Overall Performance:")
    print(f"  Topics tested: {aggregate['topics_tested']}")
    print(f"  Chain wins: {aggregate['results']['chain_wins']}")
    print(f"  Baseline wins: {aggregate['results']['baseline_wins']}")
    print(f"  Ties: {aggregate['results']['ties']}")
    print(f"  Chain win rate: {aggregate['results']['chain_win_rate']}")

    print(f"\nüìà Average Scores Across All Topics:")
    avg = aggregate['average_scores']

    print(f"\n  {'Dimension':<20} {'Chain Avg':>12} {'Baseline Avg':>12}")
    print(f"  {'-'*50}")
    for dim in ['novelty', 'depth', 'coherence', 'pedagogical', 'actionability']:
        print(f"  {dim.capitalize():<20} {avg['chain'].get(dim, 0):>12.1f} {avg['baseline'].get(dim, 0):>12.1f}")

    print(f"\nüî¨ Statistical Significance:")
    print(f"  {aggregate['statistical_significance']}")

    print(f"\nüìù Conclusion:")
    print(f"  {aggregate['conclusion']}")

    # Generate report
    print(f"\nüíæ Generating detailed report...")
    report_path = generate_report(aggregate, individual_results)
    print(f"  Report saved to: {report_path}")


def demo_quick_test():
    """Demo: Quick test function for rapid experimentation."""
    print("\n" + "=" * 70)
    print("DEMO 3: QUICK TEST")
    print("=" * 70)
    print("\nFor rapid experimentation, use the quick_test() function.")
    print("It measures one topic and prints concise results.")
    print()

    input("[Press Enter to run quick test]")

    quick_test(
        topic="Machine Learning",
        chain_function=concept_simplifier
    )


def demo_understanding_metrics():
    """Explain what each metric measures."""
    print("\n" + "=" * 70)
    print("UNDERSTANDING THE METRICS")
    print("=" * 70)

    metrics = {
        "Novelty": {
            "Measures": "Unique insights not obvious from the topic alone",
            "High score means": "The output reveals non-obvious connections and creates new understanding",
            "Why it matters": "This is emergence - insights that couldn't be specified in the original prompt"
        },
        "Depth": {
            "Measures": "Level of detail and sophistication",
            "High score means": "Thorough analysis that goes beyond surface-level treatment",
            "Why it matters": "Deep understanding requires building on previous reasoning steps"
        },
        "Coherence": {
            "Measures": "Logical flow and structure",
            "High score means": "Well-organized with clear progression of ideas",
            "Why it matters": "Chains should create narrative flow as each step builds on prior steps"
        },
        "Pedagogical": {
            "Measures": "Would a learner understand this?",
            "High score means": "Clear, accessible, with helpful examples and analogies",
            "Why it matters": "Good explanations require appropriate scaffolding"
        },
        "Actionability": {
            "Measures": "Can someone DO something with this?",
            "High score means": "Concrete takeaways and practical applications",
            "Why it matters": "Useful insights should lead to action, not just understanding"
        }
    }

    for metric, info in metrics.items():
        print(f"\n{metric.upper()}")
        print(f"  Measures: {info['Measures']}")
        print(f"  High score: {info['High score means']}")
        print(f"  Important because: {info['Why it matters']}")


def run_all_demos():
    """Run complete emergence measurement demonstration."""
    print("\n" + "=" * 70)
    print("EMERGENCE MEASUREMENT - COMPLETE DEMO")
    print("=" * 70)
    print("\nThis demonstration scientifically validates that prompt chains")
    print("unlock insights impossible from single mega-prompts.")
    print("\nWe'll measure emergence across multiple dimensions and topics")
    print("to provide statistical evidence for the framework's value.")

    demos = [
        ("Understanding Metrics", demo_understanding_metrics),
        ("Single Topic Comparison", demo_single_comparison),
        ("Quick Test Function", demo_quick_test),
        ("Batch Measurement (Statistical)", demo_batch_measurement)
    ]

    for i, (name, demo_func) in enumerate(demos, 1):
        if i > 1:  # Skip input for first demo
            input(f"\n\n[Press Enter to run Demo {i}/{len(demos)}: {name}]")
        else:
            print(f"\n\nDemo {i}/{len(demos)}: {name}")
        demo_func()

    print("\n" + "=" * 70)
    print("üéâ EMERGENCE MEASUREMENT DEMO COMPLETED!")
    print("=" * 70)
    print("\nWhat we've demonstrated:")
    print("  ‚úÖ Chains can be scientifically measured against baselines")
    print("  ‚úÖ Multiple dimensions capture different aspects of quality")
    print("  ‚úÖ Batch testing provides statistical confidence")
    print("  ‚úÖ Detailed reports document the evidence")

    print("\nHow to use this in practice:")
    print("  1. Test your chain functions against baselines")
    print("  2. Identify which dimensions chains excel at")
    print("  3. Use batch measurement for statistical validation")
    print("  4. Generate reports to share findings")

    print("\nThis provides evidence-based justification for using chains!")
    print("=" * 70)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Emergence Measurement Demo")
    parser.add_argument(
        "--demo",
        type=int,
        choices=[1, 2, 3, 4, 5],
        help="Run specific demo (1=Understanding, 2=Single, 3=Quick, 4=Batch, 5=All)"
    )

    args = parser.parse_args()

    if args.demo == 1:
        demo_understanding_metrics()
    elif args.demo == 2:
        demo_single_comparison()
    elif args.demo == 3:
        demo_quick_test()
    elif args.demo == 4:
        demo_batch_measurement()
    else:
        run_all_demos()
